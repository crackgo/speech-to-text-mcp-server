#!/usr/bin/env python3
"""
Speech-to-Text MCP Server
æ”¯æŒè¯­éŸ³è½¬æ–‡æœ¬å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½
"""

import os
import sys
import logging
from pathlib import Path
from typing import Optional
import asyncio
import threading
from datetime import datetime

# è®¾ç½® FFmpeg è·¯å¾„
if os.name == 'nt':  # Windows
    ffmpeg_path = r"C:\ProgramData\chocolatey\bin"
    if ffmpeg_path not in os.environ.get('PATH', ''):
        os.environ['PATH'] = ffmpeg_path + os.pathsep + os.environ.get('PATH', '')

# MCP imports
from mcp.server import Server
from mcp.types import Tool, TextContent

# è¯­éŸ³å¤„ç†åº“
import whisper
import torch
import subprocess
import tempfile

# å»¶è¿Ÿå¯¼å…¥ pyannote.audio ä»¥é¿å…ä¾èµ–å†²çª
# from pyannote.audio import Pipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– MCP Server
app = Server("speech-to-text-server")

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
WHISPER_MODEL = None
DIARIZATION_PIPELINE = None

# å…¨å±€çº¿ç¨‹æ± ,ç”¨äºè·Ÿè¸ªåå°ä»»åŠ¡
BACKGROUND_THREADS = []

# æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
SUPPORTED_FORMATS = [
    "mp3", "wav", "m4a", "flac", "ogg", "wma", 
    "aac", "opus", "webm", "mp4"
]


def initialize_whisper_model(model_size: str = "medium"):
    """åˆå§‹åŒ– Whisper æ¨¡å‹"""
    global WHISPER_MODEL
    if WHISPER_MODEL is None:
        logger.info(f"æ­£åœ¨åŠ è½½ Whisper {model_size} æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        WHISPER_MODEL = whisper.load_model(model_size, device=device)
        logger.info(f"Whisper æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    return WHISPER_MODEL


def initialize_diarization_pipeline():
    """åˆå§‹åŒ–è¯´è¯äººåˆ†ç¦»ç®¡é“"""
    global DIARIZATION_PIPELINE
    if DIARIZATION_PIPELINE is None:
        # å»¶è¿Ÿå¯¼å…¥ pyannote.audio ä»¥é¿å…å¯åŠ¨æ—¶çš„ä¾èµ–å†²çª
        try:
            from pyannote.audio import Pipeline
        except ImportError as e:
            raise ImportError(
                "æ— æ³•å¯¼å…¥ pyannote.audioã€‚è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–:\n"
                "pip install pyannote.audio torch torchvision torchaudio --upgrade\n"
                f"é”™è¯¯è¯¦æƒ…: {str(e)}"
            )
        
        hf_token = os.environ.get("HUGGINGFACE_TOKEN")
        if not hf_token:
            raise ValueError(
                "éœ€è¦è®¾ç½® HUGGINGFACE_TOKEN ç¯å¢ƒå˜é‡æ¥ä½¿ç”¨è¯´è¯äººåˆ†ç¦»åŠŸèƒ½ã€‚\n"
                "è¯·è®¿é—® https://huggingface.co/settings/tokens è·å– token"
            )
        
        logger.info("æ­£åœ¨åŠ è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
        DIARIZATION_PIPELINE = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        
        # å¦‚æœæœ‰ GPU,ä½¿ç”¨ GPU
        if torch.cuda.is_available():
            DIARIZATION_PIPELINE.to(torch.device("cuda"))
            logger.info("è¯´è¯äººåˆ†ç¦»æ¨¡å‹å·²åŠ è½½åˆ° GPU")
        else:
            logger.info("è¯´è¯äººåˆ†ç¦»æ¨¡å‹å·²åŠ è½½åˆ° CPU")
    
    return DIARIZATION_PIPELINE


def convert_to_wav(audio_path: str) -> str:
    """å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸º WAV æ ¼å¼"""
    audio_path_obj = Path(audio_path)
    
    # å¦‚æœå·²ç»æ˜¯ WAV æ ¼å¼,ç›´æ¥è¿”å›
    if audio_path_obj.suffix.lower() == '.wav':
        return audio_path
    
    logger.info(f"æ­£åœ¨è½¬æ¢éŸ³é¢‘æ–‡ä»¶ä¸º WAV æ ¼å¼...")
    
    # åˆ›å»ºä¸´æ—¶ WAV æ–‡ä»¶
    wav_path = audio_path_obj.with_suffix('.wav')
    
    try:
        # ä½¿ç”¨ ffmpeg è½¬æ¢(è®¾ç½®ç¼–ç é¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜)
        process = subprocess.Popen([
            'ffmpeg', '-i', str(audio_path),
            '-acodec', 'pcm_s16le',
            '-ar', '16000',
            '-ac', '1',
            '-y',  # è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶
            str(wav_path)
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8', errors='ignore')
        
        stdout, stderr = process.communicate()
        if process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, 'ffmpeg', stderr)
        
        logger.info(f"éŸ³é¢‘å·²è½¬æ¢ä¸º: {wav_path}")
        return str(wav_path)
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"éŸ³é¢‘è½¬æ¢å¤±è´¥: {e}")


def get_audio_duration(audio_path: str) -> float:
    """è·å–éŸ³é¢‘æ—¶é•¿(ç§’)"""
    try:
        result = subprocess.run([
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            str(audio_path)
        ], capture_output=True, text=True, check=True, encoding='utf-8', errors='ignore')
        
        duration = float(result.stdout.strip())
        return duration
    except Exception as e:
        raise RuntimeError(f"æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {e}")


def format_timestamp(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ—¶é—´æˆ³ HH:MM:SS.mmm"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"


def transcribe_with_whisper(audio_path: str, language: Optional[str] = None) -> dict:
    """ä½¿ç”¨ Whisper è¿›è¡Œè¯­éŸ³è¯†åˆ«"""
    model = initialize_whisper_model()
    
    logger.info(f"å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path}")
    
    # è½¬å½•å‚æ•°
    transcribe_options = {
        "task": "transcribe",
        "verbose": False,
    }
    
    if language:
        transcribe_options["language"] = language
    
    # æ‰§è¡Œè½¬å½•
    result = model.transcribe(audio_path, **transcribe_options)
    
    logger.info("è½¬å½•å®Œæˆ")
    return result


def perform_diarization(audio_path: str) -> dict:
    """æ‰§è¡Œè¯´è¯äººåˆ†ç¦»"""
    import time
    pipeline = initialize_diarization_pipeline()
    
    logger.info("å¼€å§‹è¯´è¯äººåˆ†ç¦»åˆ†æ...")
    logger.info(f"éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯WAVæ ¼å¼ï¼ˆpyannoteå¯¹WAVæ ¼å¼å¤„ç†æ›´ç¨³å®šï¼‰
    if not audio_path.lower().endswith('.wav'):
        logger.info("è½¬æ¢éŸ³é¢‘ä¸ºWAVæ ¼å¼ä»¥æé«˜å…¼å®¹æ€§...")
        audio_path = convert_to_wav(audio_path)
        logger.info(f"å·²è½¬æ¢ä¸º: {audio_path}")
    
    # è·å–éŸ³é¢‘æ—¶é•¿ç”¨äºè¿›åº¦ä¼°ç®—
    duration = get_audio_duration(audio_path)
    logger.info(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f} ç§’")
    
    # æ‰§è¡Œåˆ†ç¦»ï¼ˆè¿™ä¸€æ­¥å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰
    logger.info("æ­£åœ¨æ‰§è¡Œ pyannote.audio è¯´è¯äººåˆ†ç¦»ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    start_time = time.time()
    
    try:
        # å¯¹äºé•¿éŸ³é¢‘ï¼Œpyannoteå¯èƒ½ä¼šæœ‰tensor sizeä¸åŒ¹é…çš„é—®é¢˜
        # ä½¿ç”¨æ›´å°çš„batch size
        diarization = pipeline(audio_path)
        elapsed = time.time() - start_time
        logger.info(f"è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f} ç§’")
    except RuntimeError as e:
        if "Sizes of tensors must match" in str(e):
            logger.warning(f"é‡åˆ°tensor sizeé—®é¢˜ï¼Œå°è¯•é‡æ–°å¤„ç†: {e}")
            logger.info("ä½¿ç”¨å¤‡ç”¨å¤„ç†æ–¹æ³•...")
            # é‡æ–°åŠ è½½pipelineå¯èƒ½ä¼šè§£å†³é—®é¢˜
            global DIARIZATION_PIPELINE
            DIARIZATION_PIPELINE = None
            pipeline = initialize_diarization_pipeline()
            diarization = pipeline(audio_path)
            elapsed = time.time() - start_time
            logger.info(f"è¯´è¯äººåˆ†ç¦»å®Œæˆï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰ï¼Œè€—æ—¶: {elapsed:.1f} ç§’")
        else:
            logger.error(f"è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
            raise
    except Exception as e:
        logger.error(f"è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
        raise
    
    # å°†ç»“æœè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    speakers_timeline = []
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speakers_timeline.append({
            "start": turn.start,
            "end": turn.end,
            "speaker": speaker
        })
    
    num_speakers = len(set(item['speaker'] for item in speakers_timeline))
    logger.info(f"è¯†åˆ«åˆ° {num_speakers} ä¸ªè¯´è¯äººï¼Œå…± {len(speakers_timeline)} ä¸ªè¯­éŸ³æ®µ")
    return speakers_timeline


def merge_transcription_with_diarization(transcription: dict, diarization: list) -> str:
    """å°†è½¬å½•ç»“æœä¸è¯´è¯äººåˆ†ç¦»ç»“æœåˆå¹¶"""
    segments = transcription.get("segments", [])
    
    result_lines = []
    
    for segment in segments:
        start_time = segment["start"]
        end_time = segment["end"]
        text = segment["text"].strip()
        
        # æ‰¾åˆ°å¯¹åº”çš„è¯´è¯äºº
        speaker = "UNKNOWN"
        for dia in diarization:
            # å¦‚æœè½¬å½•ç‰‡æ®µçš„å¼€å§‹æ—¶é—´åœ¨è¯´è¯äººæ—¶é—´æ®µå†…
            if dia["start"] <= start_time <= dia["end"]:
                speaker = dia["speaker"]
                break
        
        # æ ¼å¼åŒ–è¾“å‡º
        timestamp = f"[{format_timestamp(start_time)} --> {format_timestamp(end_time)}]"
        line = f"[è¯´è¯äºº {speaker}] {timestamp}\n{text}\n"
        result_lines.append(line)
    
    return "\n".join(result_lines)


def format_simple_transcription(transcription: dict) -> str:
    """æ ¼å¼åŒ–ç®€å•è½¬å½•ç»“æœ(æ— è¯´è¯äººåˆ†ç¦»)"""
    segments = transcription.get("segments", [])
    
    result_lines = []
    for segment in segments:
        start_time = segment["start"]
        end_time = segment["end"]
        text = segment["text"].strip()
        
        timestamp = f"[{format_timestamp(start_time)} --> {format_timestamp(end_time)}]"
        line = f"{timestamp} {text}"
        result_lines.append(line)
    
    return "\n".join(result_lines)


def process_audio_in_background(
    audio_file_path: str,
    output_path: Path,
    language: Optional[str],
    enable_diarization: bool,
    duration_minutes: float
):
    """åå°å¤„ç†é•¿éŸ³é¢‘æ–‡ä»¶å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    
    # ç«‹å³åˆ›å»ºå¤„ç†æ ‡è®°æ–‡ä»¶
    marker_file = Path(output_path).with_suffix('.processing')
    try:
        with open(marker_file, 'w', encoding='utf-8') as f:
            f.write(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"éŸ³é¢‘æ–‡ä»¶: {audio_file_path}\n")
            f.write(f"çº¿ç¨‹ID: {threading.current_thread().name}\n")
    except Exception as e:
        logger.error(f"æ— æ³•åˆ›å»ºæ ‡è®°æ–‡ä»¶: {e}")
    
    # ç«‹å³è®°å½•çº¿ç¨‹å¯åŠ¨
    logger.info("="*60)
    logger.info(f"ğŸš€ åå°çº¿ç¨‹å·²è¿›å…¥å‡½æ•°")
    logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {audio_file_path}")
    logger.info(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {output_path}")
    logger.info(f"â±ï¸ éŸ³é¢‘æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿ")
    logger.info("="*60)
    
    try:
        logger.info(f"ğŸ”„ å¼€å§‹è½¬å½•å¤„ç†...")
        
        # è½¬æ¢ä¸º WAV æ ¼å¼
        wav_path = convert_to_wav(audio_file_path)
        
        # æ‰§è¡Œè½¬å½•
        transcription = transcribe_with_whisper(wav_path, language)
        
        # å¦‚æœå¯ç”¨è¯´è¯äººåˆ†ç¦»
        num_speakers = 0
        if enable_diarization:
            diarization = perform_diarization(wav_path)
            result_text = merge_transcription_with_diarization(transcription, diarization)
            num_speakers = len(set(seg["speaker"] for seg in diarization))
        else:
            result_text = format_simple_transcription(transcription)
        
        # æ·»åŠ å…ƒä¿¡æ¯
        detected_language = transcription.get("language", "unknown")
        header = f"{'='*60}\n"
        header += f"è¯­éŸ³è½¬å½•ç»“æœ\n"
        header += f"{'='*60}\n\n"
        header += f"ğŸ“ æ–‡ä»¶: {Path(audio_file_path).name}\n"
        header += f"â±ï¸ æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿ\n"
        header += f"ğŸŒ è¯­è¨€: {detected_language}\n"
        header += f"ğŸ‘¥ è¯´è¯äººåˆ†ç¦»: {'å·²å¯ç”¨' if enable_diarization else 'æœªå¯ç”¨'}\n"
        if enable_diarization and num_speakers > 0:
            header += f"ğŸ¤ è¯†åˆ«è¯´è¯äººæ•°: {num_speakers} ä½\n"
        header += f"ğŸ“… è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        header += f"\n{'='*60}\n\n"
        
        full_result = header + result_text
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_result)
        
        logger.info(f"âœ… åå°ä»»åŠ¡å®Œæˆ: {output_path}")
        
        # åˆ é™¤å¤„ç†æ ‡è®°æ–‡ä»¶
        if marker_file.exists():
            marker_file.unlink()
        
    except Exception as e:
        error_msg = f"âŒ åå°å¤„ç†å¤±è´¥: {str(e)}\n"
        logger.error(error_msg, exc_info=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(error_msg)
        
        # æ›´æ–°æ ‡è®°æ–‡ä»¶ä¸ºé”™è¯¯çŠ¶æ€
        try:
            with open(marker_file, 'a', encoding='utf-8') as f:
                f.write(f"\né”™è¯¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"é”™è¯¯ä¿¡æ¯: {str(e)}\n")
        except:
            pass


async def transcribe_audio_file(
    audio_file_path: str,
    language: Optional[str] = "zh",
    enable_diarization: bool = True  # é»˜è®¤å¼€å¯è¯´è¯äººåˆ†ç¦»
) -> str:
    """
    è½¬å½•éŸ³é¢‘æ–‡ä»¶ - ç›´æ¥è¿”å›è½¬å½•ç»“æœ
    
    Args:
        audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        language: è¯­è¨€ä»£ç  (å¦‚ "zh", "en"),é»˜è®¤è‡ªåŠ¨æ£€æµ‹
        enable_diarization: æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»
    
    Returns:
        å®Œæ•´çš„è½¬å½•æ–‡æœ¬ç»“æœ
    """
    try:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not Path(audio_file_path).exists():
            return f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨\nè·¯å¾„: {audio_file_path}"
        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        file_ext = Path(audio_file_path).suffix.lower().lstrip('.')
        if file_ext not in SUPPORTED_FORMATS:
            return f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ '{file_ext}'\n\næ”¯æŒçš„æ ¼å¼: {', '.join(SUPPORTED_FORMATS)}"
        
        # æ£€æŸ¥æ–‡ä»¶æ—¶é•¿
        duration = get_audio_duration(audio_file_path)
        duration_minutes = duration / 60
        
        if duration_minutes > 60:
            return f"âŒ é”™è¯¯: éŸ³é¢‘æ—¶é•¿ {duration_minutes:.1f} åˆ†é’Ÿè¶…è¿‡ 60 åˆ†é’Ÿé™åˆ¶"
        
        # é¢„ä¼°å¤„ç†æ—¶é—´
        estimated_time = int(duration_minutes * 1.2)  # GPU å¤§çº¦ 1.2å€æ—¶é—´
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_path = Path(audio_file_path).with_suffix('.txt')
        
        # ç«‹å³è¿”å›çŠ¶æ€ä¿¡æ¯
        status_msg = f"""âœ… è½¬å½•ä»»åŠ¡å·²å¯åŠ¨ï¼

ğŸ“ æ–‡ä»¶ä¿¡æ¯:
   - æ–‡ä»¶å: {Path(audio_file_path).name}
   - æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿ
   - æ ¼å¼: {file_ext.upper()}

âš™ï¸ å¤„ç†è®¾ç½®:
   - è¯­è¨€: {language or 'è‡ªåŠ¨æ£€æµ‹'}
   - è¯´è¯äººåˆ†ç¦»: {'æ˜¯' if enable_diarization else 'å¦'}
   - è®¾å¤‡: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}

â±ï¸ é¢„è®¡æ—¶é—´: {estimated_time} åˆ†é’Ÿ

ğŸ’¾ ç»“æœå°†ä¿å­˜åˆ°:
   {output_path}

ğŸ”„ æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™...
"""
        
        logger.info(f"å¼€å§‹è½¬å½•: {audio_file_path}")
        logger.info(f"éŸ³é¢‘æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿï¼Œé¢„è®¡å¤„ç†æ—¶é—´: {estimated_time} åˆ†é’Ÿ")
        
        # æ ¹æ®éŸ³é¢‘é•¿åº¦å†³å®šå¤„ç†æ–¹å¼
        # çŸ­éŸ³é¢‘ (â‰¤3åˆ†é’Ÿ): åŒæ­¥å¤„ç†ï¼Œç›´æ¥è¿”å›å®Œæ•´ç»“æœ
        # é•¿éŸ³é¢‘ (>3åˆ†é’Ÿ): ç«‹å³è¿”å›çŠ¶æ€ï¼Œåå°å¤„ç†å¹¶ä¿å­˜åˆ°æ–‡ä»¶
        
        if duration_minutes <= 3:
            # çŸ­éŸ³é¢‘ - åŒæ­¥å¤„ç†å¹¶ç›´æ¥è¿”å›
            logger.info("ğŸ¯ çŸ­éŸ³é¢‘ï¼ŒåŒæ­¥å¤„ç†ä¸­...")
            
            # è½¬æ¢ä¸º WAV æ ¼å¼
            wav_path = convert_to_wav(audio_file_path)
            
            # æ‰§è¡Œè½¬å½•
            transcription = transcribe_with_whisper(wav_path, language)
            
            # å¦‚æœå¯ç”¨è¯´è¯äººåˆ†ç¦»
            num_speakers = 0
            if enable_diarization:
                diarization = perform_diarization(wav_path)
                result_text = merge_transcription_with_diarization(transcription, diarization)
                num_speakers = len(set(seg["speaker"] for seg in diarization))
            else:
                result_text = format_simple_transcription(transcription)
            
            # æ·»åŠ å…ƒä¿¡æ¯
            detected_language = transcription.get("language", "unknown")
            header = f"{'='*60}\n"
            header += f"è¯­éŸ³è½¬å½•ç»“æœ\n"
            header += f"{'='*60}\n\n"
            header += f"ğŸ“ æ–‡ä»¶: {Path(audio_file_path).name}\n"
            header += f"â±ï¸ æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿ\n"
            header += f"ğŸŒ è¯­è¨€: {detected_language}\n"
            header += f"ğŸ‘¥ è¯´è¯äººåˆ†ç¦»: {'å·²å¯ç”¨' if enable_diarization else 'æœªå¯ç”¨'}\n"
            if enable_diarization and num_speakers > 0:
                header += f"ğŸ¤ è¯†åˆ«è¯´è¯äººæ•°: {num_speakers} ä½\n"
            header += f"ğŸ“… è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            header += f"\n{'='*60}\n\n"
            
            full_result = header + result_text
            
            logger.info(f"âœ… è½¬å½•å®Œæˆ")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if wav_path != audio_file_path and os.path.exists(wav_path):
                try:
                    os.remove(wav_path)
                except:
                    pass
            
            # ç›´æ¥è¿”å›å®Œæ•´ç»“æœ
            return full_result
            
        else:
            # é•¿éŸ³é¢‘ - ä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹å¤„ç†,é¿å…MCPè¶…æ—¶
            logger.info("ğŸ“ é•¿éŸ³é¢‘,ä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹å¤„ç†...")
            
            # å‡†å¤‡ç‹¬ç«‹è¿›ç¨‹çš„æ—¥å¿—æ–‡ä»¶
            log_file = output_path.with_suffix('.log')
            stderr_file = output_path.with_suffix('.stderr')
            
            # ä½¿ç”¨subprocesså¯åŠ¨ç‹¬ç«‹Pythonè¿›ç¨‹
            import subprocess
            python_exe = sys.executable  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
            script_path = Path(__file__).parent / "standalone_transcribe.py"
            
            # å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡,ç¡®ä¿HUGGINGFACE_TOKENç­‰è¢«ä¼ é€’
            env = os.environ.copy()
            
            # Windowså¹³å°ä½¿ç”¨CREATE_NO_WINDOWåˆ›å»ºåå°è¿›ç¨‹
            if os.name == 'nt':
                import subprocess
                CREATE_NO_WINDOW = 0x08000000
                creation_flags = CREATE_NO_WINDOW
            else:
                creation_flags = 0
            
            # å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
            process = subprocess.Popen(
                [
                    python_exe,
                    "-u",  # æ— ç¼“å†²è¾“å‡º
                    str(script_path),
                    audio_file_path,
                    str(output_path),
                    language or "None",
                    str(enable_diarization),
                    str(log_file)
                ],
                env=env,  # ä¼ é€’ç¯å¢ƒå˜é‡
                stdout=open(stderr_file, 'w', encoding='utf-8', buffering=1),
                stderr=subprocess.STDOUT,  # åˆå¹¶stderråˆ°stdout
                creationflags=creation_flags
            )
            
            logger.info(f"ç‹¬ç«‹è¿›ç¨‹å·²å¯åŠ¨: PID={process.pid}")
            logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
            logger.info(f"é”™è¯¯æ—¥å¿—: {stderr_file}")
            
            # ç«‹å³è¿”å›ä»»åŠ¡ä¿¡æ¯
            return f"""âœ… è½¬å½•ä»»åŠ¡å·²åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¯åŠ¨

ğŸ“ æ–‡ä»¶ä¿¡æ¯:
   - æ–‡ä»¶å: {Path(audio_file_path).name}
   - æ—¶é•¿: {duration_minutes:.1f} åˆ†é’Ÿ
   - æ ¼å¼: {file_ext.upper()}

âš™ï¸ å¤„ç†è®¾ç½®:
   - è¯­è¨€: {language or 'è‡ªåŠ¨æ£€æµ‹'}
   - è¯´è¯äººåˆ†ç¦»: {'æ˜¯' if enable_diarization else 'å¦'}
   - è®¾å¤‡: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}
   - è¿›ç¨‹ID: {process.pid}

â±ï¸ é¢„è®¡å®Œæˆæ—¶é—´: çº¦ {estimated_time} åˆ†é’Ÿå

ğŸ’¾ ç»“æœå°†ä¿å­˜åˆ°:
   {output_path}

ğŸ“‹ æŸ¥çœ‹å¤„ç†è¿›åº¦:
   æ—¥å¿—æ–‡ä»¶: {log_file}
   é”™è¯¯æ—¥å¿—: {stderr_file}

ğŸ”„ å¤„ç†å°†åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å®Œæˆ,ä¸å—MCPè¶…æ—¶é™åˆ¶ã€‚
å®Œæˆåè¯·æ‰“å¼€è¾“å‡ºæ–‡ä»¶æŸ¥çœ‹è½¬å½•ç»“æœã€‚
"""
        
    except Exception as e:
        error_msg = f"âŒ è½¬å½•å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—"
        logger.error(error_msg, exc_info=True)
        return error_msg


# å®šä¹‰ MCP å·¥å…·
@app.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="transcribe_audio",
            description=(
                "å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•ä¸ºæ–‡æœ¬ã€‚"
                "çŸ­éŸ³é¢‘(â‰¤5åˆ†é’Ÿ)ç›´æ¥è¿”å›å®Œæ•´ç»“æœï¼›"
                "é•¿éŸ³é¢‘(>5åˆ†é’Ÿ)åå°å¤„ç†å¹¶ä¿å­˜åˆ°åŒå.txtæ–‡ä»¶(é¿å…MCPè¶…æ—¶)ã€‚"
                "é»˜è®¤å¯ç”¨è¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼Œæ”¯æŒè¯†åˆ«ä¸åŒè¯´è¯äººã€‚"
                "å¤„ç†æ—¶é—´çº¦ä¸ºéŸ³é¢‘æ—¶é•¿çš„1.2å€ï¼ˆGPUåŠ é€Ÿï¼‰ã€‚"
                "æ”¯æŒæ ¼å¼: mp3, wav, m4a, flac, ogg, wma ç­‰ã€‚"
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "audio_file_path": {
                        "type": "string",
                        "description": "éŸ³é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ (ä¾‹å¦‚: C:\\Users\\username\\audio.mp3)"
                    },
                    "language": {
                        "type": "string",
                        "description": "è¯­è¨€ä»£ç ,å¦‚ 'zh' (ä¸­æ–‡), 'en' (è‹±æ–‡), 'ja' (æ—¥è¯­)ã€‚ç•™ç©ºåˆ™è‡ªåŠ¨æ£€æµ‹",
                        "default": "zh"
                    },
                    "enable_diarization": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»(è¯†åˆ«ä¸åŒè¯´è¯äºº),éœ€è¦ HUGGINGFACE_TOKEN",
                        "default": True
                    }
                },
                "required": ["audio_file_path"]
            }
        ),
        Tool(
            name="get_supported_formats",
            description="è·å–æ”¯æŒçš„éŸ³é¢‘æ ¼å¼åˆ—è¡¨",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        )
    ]


@app.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""
    
    try:
        if name == "transcribe_audio":
            audio_file_path = arguments.get("audio_file_path")
            language = arguments.get("language")
            enable_diarization = arguments.get("enable_diarization", True)
            
            if not audio_file_path:
                return [TextContent(
                    type="text",
                    text="é”™è¯¯: ç¼ºå°‘å¿…éœ€å‚æ•° 'audio_file_path'"
                )]
            
            result = await transcribe_audio_file(
                audio_file_path=audio_file_path,
                language=language,
                enable_diarization=enable_diarization
            )
            
            return [TextContent(type="text", text=result)]
        
        elif name == "get_supported_formats":
            formats_text = "æ”¯æŒçš„éŸ³é¢‘æ ¼å¼:\n" + "\n".join(f"- {fmt}" for fmt in SUPPORTED_FORMATS)
            return [TextContent(type="text", text=formats_text)]
        
        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]
    
    except Exception as e:
        error_message = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
        logger.error(error_message, exc_info=True)
        return [TextContent(type="text", text=error_message)]


async def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨ stdio ä¼ è¾“è¿è¡ŒæœåŠ¡å™¨
    from mcp.server.stdio import stdio_server
    
    async with stdio_server() as (read_stream, write_stream):
        logger.info("Speech-to-Text MCP Server å·²å¯åŠ¨")
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )


if __name__ == "__main__":
    asyncio.run(main())
